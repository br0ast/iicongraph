{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "204a0a17-1a3a-454b-88ef-cfaea1951b56",
   "metadata": {},
   "source": [
    "# Creating IICONGRAPHarco Knowledge Graph\n",
    "\n",
    "In this document, we will generate the IICONGRAPHarco Knowledge Graph by first extracting the data from ArCo, translating it (very slow process, could take more than 24 hours), and then converting it to the shortcut version of the ICON ontology. If you want to just reproduce IICONGRAPharco1.0 (the version in Zenodo, jump to section: \"Converting ArCo's data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc91540f-9557-4a27-81a7-5a17b169a169",
   "metadata": {},
   "source": [
    "## Extracting ArCo's data\n",
    "\n",
    "In this blocks, we extract ArCo's data from its [SPARQL endpoint](https://dati.beniculturali.it/sparql). This endpoint has a limit of 10000 retrievals. We know we want to extract all the entities of type HistoricOrArtisticProperty whose description starts with \"lettura iconografica:\" (transl= \"Iconographic reading:\"). Because we can only extract up to 10000 results with this method, we can divide the queries according to the first letter after the \":\" We count first all the occurrences after the first letter to make sure we don't have some with more than 10000 results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9a2304d-05ef-48d7-bd8f-66b99939ceb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SPARQLWrapper import SPARQLWrapper, JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35b2b767-9bae-455b-ad80-1384e934ab5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparql_query_setting(query, endpoint):\n",
    "    \"\"\"\n",
    "    Execute a SPARQL query on a specified endpoint and return the results in JSON format.\n",
    "\n",
    "    Parameters:\n",
    "    - query (str): The SPARQL query to be executed.\n",
    "    - endpoint (str): The endpoint URL for the SPARQL service.\n",
    "\n",
    "    Returns:\n",
    "    - dict: The result of the SPARQL query in JSON format.\n",
    "    \"\"\"\n",
    "    # Create a SPARQLWrapper instance and set the endpoint\n",
    "    sparql = SPARQLWrapper(endpoint)\n",
    "    \n",
    "    # Set the SPARQL query\n",
    "    sparql.setQuery(query)\n",
    "    \n",
    "    # Set the returned format to JSON\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    \n",
    "    # Execute the query and convert the results to JSON format\n",
    "    results = sparql.query().convert()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# this should take around 1-2 minutes\n",
    "countperletter = dict()\n",
    "arco_endpoint = \"https://dati.cultura.gov.it/sparql\"\n",
    "letters = \"a b c d e f g h i j k l m n o p q r s t u v w x y z\"\n",
    "lettersl = letters.split(\" \")\n",
    "\n",
    "for letter in lettersl:\n",
    "    arco_query = '''\n",
    "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "    PREFIX arco-cd: <https://w3id.org/arco/ontology/context-description/>\n",
    "    PREFIX arco-dd: <https://w3id.org/arco/ontology/denotative-description/>\n",
    "    PREFIX arco: <https://w3id.org/arco/ontology/arco/>\n",
    "    PREFIX dc: <http://purl.org/dc/elements/1.1/>\n",
    "\n",
    "    SELECT (COUNT(?Concept) AS ?tot) WHERE {{\n",
    "        ?Concept a <https://w3id.org/arco/ontology/arco/HistoricOrArtisticProperty> ;\n",
    "                 <https://w3id.org/arco/ontology/core/description> ?description .\n",
    "        FILTER(REGEX(?description, \"lettura iconografica: '''+letter+'''\", \"i\"))\n",
    "    }} LIMIT 10000\n",
    "    '''\n",
    "\n",
    "    # Execute the SPARQL query using the sparql_query_setting function\n",
    "    res = sparql_query_setting(arco_query, arco_endpoint)\n",
    "\n",
    "    # Extract and store the results in the countperletter dictionary\n",
    "    for result in res[\"results\"][\"bindings\"]:\n",
    "        number = result[\"tot\"][\"value\"]\n",
    "        countperletter[letter] = int(number)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5398bfd-aee7-4d58-889a-b71f4a0f9b58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 1565,\n",
       " 'b': 1261,\n",
       " 'c': 1677,\n",
       " 'd': 374,\n",
       " 'e': 169,\n",
       " 'f': 11846,\n",
       " 'g': 624,\n",
       " 'h': 11,\n",
       " 'i': 285,\n",
       " 'j': 17,\n",
       " 'k': 7,\n",
       " 'l': 543,\n",
       " 'm': 1201,\n",
       " 'n': 304,\n",
       " 'o': 147,\n",
       " 'p': 1350,\n",
       " 'q': 27,\n",
       " 'r': 1524,\n",
       " 's': 1377,\n",
       " 't': 580,\n",
       " 'u': 232,\n",
       " 'v': 1765,\n",
       " 'w': 4,\n",
       " 'x': 2,\n",
       " 'y': 3,\n",
       " 'z': 13}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countperletter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392f4dda-1cfe-4886-ac0b-6510f5385e6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdeda91-1037-4f0c-933f-a777ba63a04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We see that the only letter with more than 10000 results is \"f\",\n",
    "# therefore we only need to split the query to\n",
    "# extract the data into two parts for this letter\n",
    "\n",
    "# We extract the URIs or artworks, their description and the dating\n",
    "\n",
    "#data_for_letter = dict()\n",
    "data_for_letter_l = [] #list of tuples is better for the translation part later\n",
    "#Extract data for every letter and the first 8000 for \"f\"\n",
    "# it should take 2-3 minutes\n",
    "for letter in lettersl:\n",
    "    arco_query = '''\n",
    "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "    PREFIX arco-cd: <https://w3id.org/arco/ontology/context-description/>\n",
    "    PREFIX arco-dd: <https://w3id.org/arco/ontology/denotative-description/>\n",
    "    PREFIX arco: <https://w3id.org/arco/ontology/arco/>\n",
    "    PREFIX dc: <http://purl.org/dc/elements/1.1/>\n",
    "\n",
    "    select distinct ?Concept ?description ?date where {\n",
    "\n",
    " ?Concept a <https://w3id.org/arco/ontology/arco/HistoricOrArtisticProperty> ;\n",
    "<https://w3id.org/arco/ontology/core/description> ?description .\n",
    "FILTER(regex(?description, \"lettura iconografica: '''+letter+'''\", \"i\"))\n",
    "OPTIONAL { ?Concept dc:date ?date }\n",
    "} LIMIT 8000\n",
    "    \n",
    "    '''     \n",
    "    res = sparql_query_setting(arco_query, arco_endpoint)\n",
    "    for result in res[\"results\"][\"bindings\"]:\n",
    "        concept = result[\"Concept\"][\"value\"]\n",
    "        description = result[\"description\"][\"value\"]\n",
    "        date = result[\"date\"][\"value\"]\n",
    "        #data_for_letter[concept] = {\"description\":description, \"date\":date}\n",
    "        data_for_letter_l.append((concept, description, date))\n",
    "\n",
    "#take the remaining of \"F\"\n",
    "arco_query = '''\n",
    "        PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "        PREFIX arco-cd: <https://w3id.org/arco/ontology/context-description/>\n",
    "        PREFIX arco-dd: <https://w3id.org/arco/ontology/denotative-description/>\n",
    "        PREFIX arco: <https://w3id.org/arco/ontology/arco/>\n",
    "        PREFIX dc: <http://purl.org/dc/elements/1.1/>\n",
    "    \n",
    "        select distinct ?Concept ?description ?date where {\n",
    "    \n",
    "     ?Concept a <https://w3id.org/arco/ontology/arco/HistoricOrArtisticProperty> ;\n",
    "    <https://w3id.org/arco/ontology/core/description> ?description .\n",
    "    FILTER(regex(?description, \"lettura iconografica: '''+\"f\"+'''\", \"i\"))\n",
    "    OPTIONAL { ?Concept dc:date ?date }\n",
    "    } OFFSET 8000 LIMIT 4000 '''\n",
    "res = sparql_query_setting(arco_query, arco_endpoint)\n",
    "for result in res[\"results\"][\"bindings\"]:\n",
    "    concept = result[\"Concept\"][\"value\"]\n",
    "    description = result[\"description\"][\"value\"]\n",
    "    date = result[\"date\"][\"value\"]\n",
    "    #data_for_letter[concept] = {\"description\":description, \"date\":date}\n",
    "    data_for_letter_l.append((concept, description, date))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f8bcef-b43f-4de9-815c-01af5318d033",
   "metadata": {},
   "source": [
    "### First Cleaning\n",
    "\n",
    "Methodology for cleaning: user choices according to the category.\n",
    "**Hint**: \"Nomi\" and \"Luoghi\" does not contain useful information. If you want to create IICONGRAPHarco you can skip until **converting ArCo's Data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7dcc2d-d0e1-4807-ba27-633e7fe82318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set of types that should be excluded\n",
    "excluded = set()\n",
    "\n",
    "# Set of types that have been checked and confirmed by the user\n",
    "checked = set()\n",
    "\n",
    "def reconstruct(text):\n",
    "    \"\"\"\n",
    "    Reconstructs a string by processing and checking its parts.\n",
    "\n",
    "    Args:\n",
    "    - text (str): The input string to be reconstructed.\n",
    "\n",
    "    Returns:\n",
    "    - str: The reconstructed string if successful, otherwise False.\n",
    "    \"\"\"\n",
    "    # Check if the last character is a period and split the text accordingly\n",
    "    if text[-1] == \".\":\n",
    "        parts = text[:-1].split(\".\")\n",
    "    else:\n",
    "        parts = text.split(\".\")\n",
    "\n",
    "    newtext = \"\"\n",
    "\n",
    "    # Iterate over parts and process each one\n",
    "    for part in parts:\n",
    "        # Split the part into type and text\n",
    "        type_text = part.split(\":\")\n",
    "\n",
    "        # Check if the format is correct\n",
    "        if len(type_text) != 2:\n",
    "            return False\n",
    "\n",
    "        # Check if the type should be excluded\n",
    "        if type_text[0] in excluded:\n",
    "            continue\n",
    "        # Check if the type has been previously checked\n",
    "        elif type_text[0] in checked:\n",
    "            newtext += \":\".join(type_text) + \".\"\n",
    "        else:\n",
    "            print(type_text[0])\n",
    "            a = input(\"Is this good?\")\n",
    "            # Ask the user if the type is good\n",
    "            if a == \"y\":\n",
    "                checked.add(type_text[0])\n",
    "                newtext += \":\".join(type_text) + \".\"\n",
    "            else:\n",
    "                excluded.add(type_text[0])\n",
    "                continue\n",
    "\n",
    "    # Check if the reconstructed string is not empty\n",
    "    if len(newtext) > 1:\n",
    "        # Add a period at the end if needed\n",
    "        if newtext[-1] != \".\":\n",
    "            newtext += \".\"\n",
    "        return newtext\n",
    "    else:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6137c0ef-a4a2-4433-a8af-e68a2c9aefe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ico_clean_1 = list()\n",
    "for el in data_for_letter_l_ico:\n",
    "    resu = reconstruct(el[1])\n",
    "    if resu is False:\n",
    "        continue\n",
    "    else:\n",
    "        data_ico_clean_1.append((el[0], resu, el[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05246dfd-4956-4dd4-af39-db8012ef1996",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6303532-54f5-4096-a22a-d2e75079a378",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e6b65ef-3d58-4ec3-be8e-c333cd82b950",
   "metadata": {},
   "source": [
    "## Translating ArCo's data\n",
    "\n",
    "This step could take more than 24H, if you want to recreate IICONGRAPHarco1.0, you can skip this step and go to **Converting ArCo's data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca640e37-56c3-4e8b-b46d-bc213b46c320",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_translator import (GoogleTranslator,\n",
    "                             ChatGptTranslator,\n",
    "                             MicrosoftTranslator,\n",
    "                             PonsTranslator,\n",
    "                             LingueeTranslator,\n",
    "                             MyMemoryTranslator,\n",
    "                             YandexTranslator,\n",
    "                             PapagoTranslator,\n",
    "                             DeeplTranslator,\n",
    "                             QcriTranslator,\n",
    "                             single_detection,\n",
    "                             batch_detection)\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637c20f9-f6e9-4c79-ac25-c5968ad13b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store translated elements\n",
    "arcofinal = []\n",
    "\n",
    "# Iterate over elements in the data_ico_clean_1 list\n",
    "for el in tqdm(data_ico_clean_1):\n",
    "    try:\n",
    "        # Check if the index is within the range of arco_transl list\n",
    "        if data_ico_clean_1.index(el) >= len(arcofinal):\n",
    "            # Extract the text to be translated\n",
    "            text = el[1]\n",
    "\n",
    "            # Translate the text from Italian to English using Google Translator\n",
    "            t2 = GoogleTranslator(source=\"it\", target=\"en\").translate(text=text)\n",
    "\n",
    "            # Add a delay to avoid excessive API requests\n",
    "            time.sleep(3)\n",
    "\n",
    "            # Append the translated element to arco_transl list\n",
    "            arcofinal.append((el[0], t2, el[2]))\n",
    "    except:\n",
    "        # Handle translation failure\n",
    "        print(\"Translation failed at element number \" + str(len(arcofinal) + 1) + \" out of \" + str(len(data_ico_clean_1)))\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7650bffe-f5c4-4438-910b-fcb3e7ea9c74",
   "metadata": {},
   "source": [
    "## Converting ArCo's data\n",
    "\n",
    "In this step, we will convert ArCo's triples to the structure of ICON 2.0 using "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23832d00-e6ef-41e2-9b75-c4ea173f79cf",
   "metadata": {},
   "source": [
    "### IF YOU SKIPPED THE PREVIOUS STEPS\n",
    "\n",
    "You can import the arco_upload.p file via pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "807d8e06-c439-40a1-9c1d-44909694f08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"arco_upload.p\", \"rb\") as input_file:\n",
    "    arcofinal = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fd4933-48c2-43c2-bf77-1f4411135035",
   "metadata": {},
   "source": [
    "### Preliminary steps\n",
    "\n",
    "We first parse HyperReal and we create some secondary functions to handle the alignment between ArCo's elements and HyperReal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5833b01-7180-477f-a1c2-c7adcf2a3aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.parse\n",
    "import rdflib\n",
    "from rdflib import URIRef, BNode, Literal, Graph, Namespace, ConjunctiveGraph\n",
    "from rdflib.namespace import CSVW, DC, DCAT, DCTERMS, DOAP, FOAF, ODRL2, ORG, OWL, \\\n",
    "                           PROF, PROV, RDF, RDFS, SDO, SH, SKOS, SOSA, SSN, TIME, \\\n",
    "                           VOID, XMLNS, XSD\n",
    "from rdflib import namespace\n",
    "import pandas\n",
    "\n",
    "from rdflib import Graph, Namespace, RDFS\n",
    "\n",
    "# Create an instance of the Graph\n",
    "hr = Graph()\n",
    "\n",
    "# Parse the RDF data from the specified URL in Turtle format\n",
    "hr.parse(\"https://raw.githubusercontent.com/br0ast/simulationontology/main/KG/kg.ttl\", format=\"ttl\")\n",
    "\n",
    "# Define namespaces for easier access\n",
    "sim_on = \"https://w3id.org/simulation/ontology/\"\n",
    "\n",
    "sim_n = Namespace(sim_on)  # Replace with the actual URI\n",
    "hr.bind(\"sim\", sim_n)\n",
    "\n",
    "hrdata = \"https://w3id.org/simulation/data/\"\n",
    "hrd = Namespace(hrdata)\n",
    "hr.bind(\"hr\", hrd)\n",
    "\n",
    "# Set to store unique types\n",
    "setoftypes = set()\n",
    "\n",
    "# Iterate through objects with the predicate sim_n.hasSimulacrum\n",
    "for o in hr.objects(None, sim_n.hasSimulacrum, None):\n",
    "    # Retrieve labels for each object\n",
    "    for lab in hr.objects(o, RDFS.label, None):\n",
    "        # Check if the label contains \"(\"\n",
    "        if \"(\" in str(lab):\n",
    "            # Add the label to the set\n",
    "            setoftypes.add(str(lab))\n",
    "\n",
    "# Create a set to store types\n",
    "setoftypes2 = set()\n",
    "\n",
    "# Iterate over elements in setoftypes\n",
    "for el in setoftypes:\n",
    "    # Extract the type from the element\n",
    "    typ = el.split(\"(\")[1].split(\")\")[0]\n",
    "    setoftypes2.add(typ)\n",
    "\n",
    "# Remove types with spaces in the set\n",
    "setoftypes2 = {el for el in setoftypes2 if \" \" not in el}\n",
    "\n",
    "def combinewithtype(string):\n",
    "    \"\"\"\n",
    "    Combine a string with each type from setoftypes2.\n",
    "\n",
    "    Args:\n",
    "    - string (str): The input string.\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of strings where each element is the input string combined with a type.\n",
    "    \"\"\"\n",
    "    # Create a list to store combined strings\n",
    "    listoftypes = []\n",
    "\n",
    "    # Iterate over types in setoftypes2\n",
    "    for typ in setoftypes2:\n",
    "        # Create a new string by combining the input string and the title-cased type\n",
    "        new_string = string + to_camel_case(typ).title()        \n",
    "        # Append the new string to the list\n",
    "        listoftypes.append(new_string)\n",
    "    # Return the list of combined strings\n",
    "    return listoftypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042fcbc2-8b0f-4c75-9314-95af0e09280d",
   "metadata": {},
   "source": [
    "#### Auxilliary function for the camelCase URIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2c536e3-8adf-49de-9c2a-1ceac65ce370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def to_camel_case(input_string):\n",
    "    \"\"\"\n",
    "    Convert a string to CamelCase.\n",
    "\n",
    "    Args:\n",
    "    - input_string (str): The input string.\n",
    "\n",
    "    Returns:\n",
    "    - str: The CamelCase version of the input string.\n",
    "    \"\"\"\n",
    "    # Remove punctuation from the input string\n",
    "    input_string = input_string.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Split the input string into words using space and underscore as delimiters\n",
    "    words = re.split(r'[_\\s]+', input_string)\n",
    "\n",
    "    # Capitalize the first letter of each word (except the first word)\n",
    "    camel_words = [words[0].lower()] + [word.capitalize() for word in words[1:]]\n",
    "\n",
    "    # Join the words together to form the CamelCase string\n",
    "    camel_case_string = ''.join(camel_words)\n",
    "\n",
    "    return camel_case_string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf0cd9b-eaf8-4df7-ad7b-0a3775720bbe",
   "metadata": {},
   "source": [
    "### Main Symbolism Function / Match function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af039994-8991-4849-a6a3-8d0e9f1a03d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_found = set()\n",
    "\n",
    "def add_symb(graph, artwork_uri, label):\n",
    "    \"\"\"\n",
    "    Add symbolic information to the knowledge graph based on the given label.\n",
    "\n",
    "    Args:\n",
    "    - graph: The RDF graph to which information will be added.\n",
    "    - artwork_uri (str): URI of the artwork.\n",
    "    - label (str): Label for which symbolic information is to be added.\n",
    "\n",
    "    Returns:\n",
    "    - str: A string describing the symbolic information added to the graph.\n",
    "    \"\"\"\n",
    "    string_to_return = []\n",
    "    possible_labels = combinewithtype(to_camel_case(label))\n",
    "    possible_labels.append(to_camel_case(label))\n",
    "    \n",
    "    if label[-1] == \"s\":\n",
    "        label2 = label[:-1]\n",
    "        possible_labels.extend(combinewithtype(to_camel_case(label2)))\n",
    "        possible_labels.append(to_camel_case(label2))\n",
    "    \n",
    "    for lab in possible_labels:\n",
    "        if lab not in not_found:\n",
    "            if (None, sim_n.hasSimulacrum, URIRef(hrdata + lab)) in hr:\n",
    "                simumu = hrdata + lab\n",
    "                resu = hr.query(\"\"\"\n",
    "                                prefix kb: <https://w3id.org/simulation/data/>\n",
    "                                prefix owl: <http://www.w3.org/2002/07/owl#>\n",
    "                                prefix prov: <http://www.w3.org/ns/prov#>\n",
    "                                prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "                                prefix sim: <https://w3id.org/simulation/ontology/>\n",
    "                                prefix wn: <http://wordnet-rdf.princeton.edu/lemma/>\n",
    "                                select ?simulation ?context ?rc ?rctype ?simulationtype ?contextLabel ?rcLabel where {\n",
    "                                    values ?rctype {sim:hasRealityCounterpart sim:elicitedRealityCounterpart sim:preventedRealityCounterpart sim:easedRealityCounterpart\n",
    "                                    sim:healedRealityCounterpart sim:restoredRealityCounterpart}\n",
    "                                    ?simulation sim:hasSimulacrum <\"\"\" + simumu + \"\"\">;\n",
    "                                    ?rctype ?rc;\n",
    "                                    sim:hasContext ?context;\n",
    "                                    a ?simulationtype .\n",
    "                                    ?context rdfs:label ?contextLabel .\n",
    "                                    ?rc rdfs:label ?rcLabel .\n",
    "                                    FILTER (?context != kb:generalOrUnknown)\n",
    "                                }\"\"\")\n",
    "                \n",
    "                for el in resu:\n",
    "                    if el[0]:\n",
    "                        graph.add((arturi, icon_n.iconographicallyDepicts, URIRef(el[0])))\n",
    "                        graph.add((URIRef(el[0]), RDF.type, URIRef(el[4])))\n",
    "                        graph.add((URIRef(el[0]), sim_n.hasSimulacrum, URIRef(simumu)))\n",
    "                        graph.add((URIRef(el[0]), URIRef(el[3]), URIRef(el[2])))\n",
    "                        graph.add((URIRef(el[0]), sim_n.hasContext, URIRef(el[1])))\n",
    "                        \n",
    "                        if str(el[4]) == \"https://w3id.org/simulation/ontology/ProtectionSimulation\":\n",
    "                            string_to_return.append(\"symbolic protection against \" + str(el[6]) + \" in a \" + str(el[5]) + \" cultural context\")\n",
    "                        elif str(el[4]) == \"https://w3id.org/simulation/ontology/AttributeSimulation\":\n",
    "                            string_to_return.append(\"symbolic attribute of \" + str(el[6]) + \" in a \" + str(el[5]) + \" cultural context\")\n",
    "                        elif str(el[4]) == \"https://w3id.org/simulation/ontology/HealingSimulation\":\n",
    "                            string_to_return.append(\"symbolic cure for \" + str(el[6]) + \" in a \" + str(el[5]) + \" cultural context\")\n",
    "                        elif str(el[4]) == \"https://w3id.org/simulation/ontology/AssociationSimulation\":\n",
    "                            string_to_return.append(\"symbolically associated with \" + str(el[6]) + \" in a \" + str(el[5]) + \" cultural context\")\n",
    "                        else:\n",
    "                            string_to_return.append(\"symbol of \" + str(el[6]) + \" in a \" + str(el[5]) + \" cultural context\")\n",
    "                        \n",
    "            else:\n",
    "                not_found.add(lab)\n",
    "    \n",
    "    if len(string_to_return) > 0:\n",
    "        result = \"(\" + \", \".join(string_to_return) + \")\"\n",
    "        return result\n",
    "    else:\n",
    "        return \"nope\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ae61e1-791d-4088-be13-895797ac47b7",
   "metadata": {},
   "source": [
    "### KG construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba91c31-7ab7-4245-966b-b2f3b5e284ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from rdflib import ConjunctiveGraph, RDF, RDFS, Literal, URIRef, Namespace\n",
    "\n",
    "# Define namespaces\n",
    "icon = \"https://w3id.org/icon/ontology/\"\n",
    "uarco = \"https://w3id.org/iicongraph/data/\"\n",
    "dolce = \"http://www.ontologydesignpatterns.org/ont/dul/DUL.owl#\"\n",
    "sim_on = \"https://w3id.org/simulation/ontology/\"\n",
    "hrdata = \"https://w3id.org/simulation/data/\"\n",
    "arcor = \"https://w3id.org/arco/resource/HistoricOrArtisticProperty/\"\n",
    "hrd = Namespace(hrdata)\n",
    "icon_n = Namespace(icon)\n",
    "uarcon = Namespace(uarco)\n",
    "arcorn = Namespace(arcor)\n",
    "dolcen = Namespace(dolce)\n",
    "sim_n = Namespace(sim_on)\n",
    "\n",
    "# Bind namespaces to the RDF graph\n",
    "g = ConjunctiveGraph()\n",
    "g.bind(\"hr\", hrd)\n",
    "g.bind(\"icon\", icon_n)\n",
    "g.bind(\"dul\", dolcen)\n",
    "g.bind(\"iig\", uarcon)\n",
    "g.bind(\"sim\", sim_n)\n",
    "g.bind(\"arcor\", arcorn)\n",
    "\n",
    "# List to store complex cases\n",
    "complex = []\n",
    "\n",
    "for arco_tup in tqdm(arcofinal):\n",
    "    if \";\" in arco_tup[1] and \".\" in arco_tup[1]:\n",
    "        arturi = URIRef(arco_tup[0])\n",
    "        g.add((arturi, RDF.type, icon_n.Artwork))\n",
    "        category_text = arco_tup[1].replace(\"St.\", \"Saint\")\n",
    "        category_text = category_text.split(\".\")\n",
    "        \n",
    "        for cat_t in category_text:\n",
    "            cat = cat_t.split(\":\")[0].lower()\n",
    "            if \"product category\" in cat or \"event type\" in cat:\n",
    "                text = cat_t.split(\":\")[1].strip()\n",
    "                iconologicals = text.split(\";\")\n",
    "                iconologicals = [\"promotion of \"+ word.strip() for word in iconologicals]\n",
    "                iconologicals_camel = [to_camel_case(word) for word in iconologicals]\n",
    "                for i, el in enumerate(iconologicals_camel):\n",
    "                    g.add((arturi, icon_n.iconologicallyRepresents, URIRef(uarco + el)))\n",
    "                    g.add((URIRef(uarco + el), RDFS.label, Literal(iconologicals[i])))\n",
    "            else:\n",
    "                if \":\" in cat_t:\n",
    "                    text = cat_t.split(\":\")[1].strip()\n",
    "                    text_reading_split = text.split(\";\")\n",
    "                    text_reading_split = [word.strip() for word in text_reading_split]\n",
    "                    \n",
    "                    if check_no_capital(text_reading_split) is True:\n",
    "                        if \"allegory\" in icon_reading or \"symbol\" in text:\n",
    "                            complex.append(arco_tup)\n",
    "                        else:\n",
    "                            text_reading_split_camel = [to_camel_case(word) for word in text_reading_split]\n",
    "                            for i, word in enumerate(text_reading_split):\n",
    "                                if len(word) > 1:\n",
    "                                    if check_no_capital_el(word) is True:\n",
    "                                        g.add((arturi, icon_n.iconographicallyDepicts, URIRef(uarco + text_reading_split_camel[i])))\n",
    "                                        g.add((URIRef(uarco + text_reading_split_camel[i]), RDFS.label, Literal(word)))\n",
    "                                        symbolism = add_symb(g, arturi, word)\n",
    "                                    else:\n",
    "                                        g.add((arturi, icon_n.preiconographicallyDepicts, URIRef(uarco + text_reading_split_camel[i])))\n",
    "                                        g.add((URIRef(uarco + text_reading_split_camel[i]), RDFS.label, Literal(word)))\n",
    "                                        symbolism = add_symb(g, arturi, word)\n",
    "                    else:\n",
    "                        if \"allegory\" in icon_reading or \"symbol\" in text:\n",
    "                            complex.append(arco_tup)\n",
    "                        else:\n",
    "                            text_reading_split_camel = [to_camel_case(word) for word in text_reading_split]\n",
    "                            for i, word in enumerate(text_reading_split):\n",
    "                                if len(word) > 1:\n",
    "                                    g.add((arturi, icon_n.preiconographicallyDepicts, URIRef(uarco + text_reading_split_camel[i])))\n",
    "                                    g.add((URIRef(uarco + text_reading_split_camel[i]), RDFS.label, Literal(word)))\n",
    "                                    symbolism = add_symb(g, arturi, word)\n",
    "        '''icon_reading = first_half.split(\":\")[1]\n",
    "        icon_reading_split = icon_reading.split(\";\")\n",
    "        icon_reading_split = [word.strip() for word in icon_reading_split]\n",
    "        if check_no_capital(icon_reading_split) is True:\n",
    "            if \"allegory\" in icon_reading or \"symbol\" in icon_reading:\n",
    "                complex.append(el)\n",
    "        else:\n",
    "            if \"allegory\" in icon_reading or \"symbol\" in icon_reading:\n",
    "                complex.append(el)'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
